{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex3.1 \n",
    "\n",
    "**Q)** Exercise 3.1 Devise three example tasks of your own that fit into the MDP framework, identifying for each its states, actions, and rewards. Make the three examples as *different* from each other as possible. The framework is abstract and flexible and can be applied in many different ways. Stretch its limits in some way in at least one of your examples.\n",
    "\n",
    "____\n",
    "\n",
    "**Ans)** \n",
    "* A game of chess in which the states are the positions of the pieces on the board. The actions are a legal move that you can make in the game. And the rewards can be +n each time you take an opponents piece, and -n when the opponent takes yours. And a final reward for winning or losing or drawing. For winning or losing, the rewards could be incorporated into the +n, where for the king, it's infinity or something. \n",
    "* An agent whose task is to write film scripts. The state it's in, is the current lines that it has written. It's actions can be to write a word down, start a new paragraph, create a new character, change the location in the script currently etc. It would get a reward for each sentence that it strings together that makes grammatical sense. Bonus rewards for writing things that are entertaining, humourous, suspensful etc maybe(?. Would need some way to assess these attributes. Could be made more minute where it writes single characters instead of words and gets a reward for writing coherent words and then a reward for stringing together a sentence and so on.\n",
    "* An agent whose job is to save the world from global warming and pollution. The states could be the current temperature of the Earth, the amount of funding we are currently putting into climate change research, the number of people recycling, the amount of fossil fuels we use, amount of renewable fuels we use etc. The actions can be to change some of these things, like invest more into research, promote recycling, invest more in renewable clean sources of energy etc. The rewards can be +n for each time it increases fundings for research, or increases the number of people who are using clean sources of energy etc, -n for each time there's more pollution or the temperature goes up etc. And a final reward of $ -\\infty $ if all humans die, and $ +\\infty $ if it's able to find a way to save us. Question is, would we want to wait that long? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex3.2\n",
    "**Q)** Exercise 3.2 Is the MDP framework adequate to usefully represent all goal-directed learning tasks? Can you think of any clear exceptions?\n",
    "\n",
    "____\n",
    "\n",
    "**Ans)** The agent requires knowledge of all actions that it can take in any given state, this might not always be true. If there's no immediate reward for the agent then how do we characterise that? A 0 reward would mean that we can characterise the action and have determined it's worth, but what if we aren't aware of how much the reward is worth or it's a highly subjective notion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex 3.3 \n",
    "\n",
    "**Q)** Exercise 3.3 Consider the problem of driving. You could define the actions in terms of the accelerator, steering wheel, and brake, that is, where your body meets the machine. Or you could define them farther out—say, where the rubber meets the road, considering your actions to be tire torques. Or you could define them farther in—say, where your brain meets your body, the actions being muscle twitches to control your limbs. Or you could go to a really high level and say that your actions are your choices of where to drive. What is the right level, the right place to draw the line between agent and environment? On what basis is one location of the line to be preferred over another? Is there any fundamental reason for preferring one location over another, or is it a free choice?\n",
    "\n",
    "____\n",
    "\n",
    "**Ans)** There is no right level or place to draw the line between agent and environment. It comes down to what your goal is, and what you are trying to achieve. If I'm trying to create an agent that can drive a car by itself then my main goal is to able to create an agent that can avoid traffic accidents, that can control the car and pick the route that will take us to destination. As we go to a higher and higher level, we are abstracting the problem further and identifying subgoals. We need to remain at a level that helps us acheive our main goal. So in the car example, it would be good to keep it at the level where in the agent can control the accelerator, steering wheel and brake. The mechanics of how it does that (controlling the limbs etc), isn't that important to me because it's a subgoal. This will help ensure that we are focusing on solving the right problem, and also helps reduce the state and action space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex 3.4 \n",
    "\n",
    "**Q)** Exercise 3.4 Give a table analogous to that in Example 3.3, but for p(s' , r | s, a). It should have columns for s, a, s', r, and p(s',r|s,a), and a row for every 4-tuple for which p(s',r|s,a) > 0.\n",
    "\n",
    "___ \n",
    "\n",
    "**Ans)** \n",
    "\n",
    "s       |  a       | s'     | r             | $p(s', r|s,a)$ \n",
    "---     |---       |---     |---            |--- \n",
    "high    | search   | high   | ${r_{search}}$|$\\alpha$     \n",
    "high    | search   | low    | ${r_{search}}$|$1 -\\alpha$   \n",
    "low     | search   | high   |     -3        |$ 1- \\beta $  \n",
    "\n",
    "and so on \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex3.5\n",
    "\n",
    "**Q)** Exercise 3.5 The equations in Section 3.1 are for the continuing case and need to be modified (very slightly) to apply to episodic tasks. Show that you know the modifications needed by giving the modified version of (3.3).\n",
    "\n",
    "____ \n",
    "\n",
    "**Ans)** \n",
    "$$\\sum_{\\substack{s' \\in S}} \\sum_{\\substack{r \\in R}} p(s',r|s,a) = 1, \\quad \\forall  s \\in S ,\\; \\forall a \\in A(s) \\qquad (3.3) $$\n",
    "\n",
    "Modified - \n",
    "\n",
    "$$\\sum_{\\substack{s' \\in S^+}} \\sum_{\\substack{r \\in R}} p(s',r|s,a) = 1, \\quad \\forall  s \\in S ,\\; \\forall a \\in A(s) \\qquad (3.3) $$\n",
    "\n",
    "$S^+$ is the set of all nonterminal states plus terminal states "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex3.6\n",
    "**Q)** Exercise 3.6 Suppose you treated pole-balancing as an episodic task but also used discounting, with all rewards zero except for -1 upon failure. What then would the return be at each time? How does this return di↵er from that in the discounted, continuing formulation of this task?\n",
    "\n",
    "_____\n",
    "\n",
    "**Ans)**\n",
    "$$ G_t = \\gamma^{T-(t+1)}$$   \n",
    "\n",
    "**Not sure yet** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex3.7 \n",
    "**Q)** Exercise 3.7 Imagine that you are designing a robot to run a maze. You decide to give it a reward of +1 for escaping from the maze and a reward of zero at all other times. The task seems to break down naturally into episodes—the successive runs through the maze—so you decide to treat it as an episodic task, where the goal is to maximize expected total reward (3.7). After running the learning agent for a while, you find that it is showing no improvement in escaping from the maze. What is going wrong? Have you effectively communicated to the agent what you want it to achieve?\n",
    "\n",
    "____ \n",
    "\n",
    "**Ans)** It's possible that the agent hasn't been able to find the exit of the maze within the time limit of the episode, and as a result it doesn't know that there exists a reward better than 0. It hasn't been exploring all it's options essentially, and it's also very easy for it to end up in a loop where it keeps going around the same states.  \n",
    "\n",
    "A way to fix that would be, to give a -1 reward for each time step, that would encourage the agent to find new paths and explore more, because if it kept going to the same states, there expected value would continuosly decrease.   \n",
    "\n",
    "In the first case we didn't effectively communicate to the agent what we wanted because we didn't push it enough to find a better reward. In a way you can think of it as allowing mediocrity.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex3.8\n",
    "**Q)** Exercise 3.8 Suppose $\\gamma$ = 0.5 and the following sequence of rewards is received R1 = -1, R2 =2, R3 =6, R4 =3, and R5 =2, with T =5. WhatareG0,G1,...,G5? Hint: Work backwards.\n",
    "\n",
    "____\n",
    "\n",
    "**Ans)** $$ G_5 = 0 $$\n",
    "___\n",
    "$$ G_4 = R_5 + \\gamma G_5 = 2 + 0 = 2 $$\n",
    "___\n",
    "$$ G_3 = R_4 + \\gamma G_4 = 3 + 0.5*2 = 4 $$\n",
    "___\n",
    "$$ G_2 = R_3 + \\gamma G_3 = 6 + 0.5*4 = 8 $$\n",
    "___\n",
    "$$ G_1 = R_2 + \\gamma G_2 = 2 + 0.5*8 = 6 $$\n",
    "___\n",
    "$$ G_0 = R_1 + \\gamma G_1 = -1 + 0.5*6 = 2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex3.9\n",
    "**Q)** Exercise 3.9 Suppose $\\gamma$ = 0.9 and the reward sequence is R1 = 2 followed by an infinite sequence of 7s. What are G1 and G0?\n",
    "\n",
    "____\n",
    "\n",
    "**Ans)**  $$G_1 = \\sum_{k=0}^{\\infty} \\gamma^k 7 = 7 \\sum_{k=0}^{\\infty} \\gamma^k = 7 \\frac{1}{1-\\gamma} = 7 \\frac{1}{1-0.9} = 70$$\n",
    "____\n",
    "$$G_0 = R_1 + \\gamma G_2 = 2 + 0.9*70 = 65$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex3.10\n",
    "**Q)** Exercise 3.10 Prove the second equality in (3.10).\n",
    "\n",
    "___\n",
    "**Ans)**  For non-zero constant reward +1 and if $\\gamma < 1$\n",
    "$$G_t = \\sum_{k=0}^{\\infty} \\gamma^k  = \\frac{1}{1-\\gamma} \\qquad  (3.10)$$\n",
    "___\n",
    "\n",
    "$$G_t = 1 + \\gamma + \\gamma^2 + .... =  \\sum_{k=0}^{\\infty} \\gamma^k $$\n",
    "\n",
    "We know sum of infinite GP for a common ratio, r < 1 is $\\frac{1}{1-r}$\n",
    "\n",
    "Therefore 3.10 holds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex3.11\n",
    "**Q)** \n",
    "\n",
    "___\n",
    "**Ans)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
